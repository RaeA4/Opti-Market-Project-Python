# -*- coding: utf-8 -*-
"""OptiMarket_Membership_Prediction.ipynb

Automatically generated by Colab.


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load datasets
df = pd.read_excel('-----/2025 Summer DATA642 Group Project.xlsx', header=0)

# Shape of the data
print(df.shape)

df.rename(columns={'Year_Birth 2': 'Year_Birth'}, inplace=True)

# First few rows
print(df.head())

# Data types and non-null counts
print(df.info())

# check for NA in the dataset
df.isnull().sum()

"""As we can see fromt the results above there are 24 missing values in income, so instead of ommiting them, we will use the median to substitue the missing values"""

df['Income'].fillna(df['Income'].median(), inplace=True)

# check for duplicates
df.duplicated().sum()

"""#Data Engineering"""

# Customer Age instead of Year of Birth
df['Customer_Age'] = 2025 - df['Year_Birth']

# Combine the number of minors at home by summing the number of kids at home and number of teens at home
df['Total_Kids'] = df['Kidhome'] + df['Teenhome']

# Combine the amount of spent in all categories
df['Total_Spent'] = df[['MntWines', 'MntFruits', 'MntMeatProducts','MntFishProducts', 'MntSweetProducts', 'MntVegProds']].sum(axis=1)

#For example:
df[['Customer_Age', 'Total_Kids', 'Total_Spent']].head()

# Create target variable that combines all any accepted campaign (Overall response)
df['AcceptedAnyCampaign'] = df[['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3',
                                'AcceptedCmp4', 'AcceptedCmp5']].sum(axis=1).apply(lambda x: 1 if x > 0 else 0)

# Drop the columns that are already used, to make the dataset more convenient
df.drop(columns=['ID', 'Year_Birth','Dt_Customer','AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3','AcceptedCmp4', 'AcceptedCmp5'], inplace=True)

# Save to CSV
df.to_csv("transformed_optimarket_data.csv", index=False)

"""# Descriptive Analysis"""

df.describe()

# 1. Calculate IQR bounds
Q1 = df['Income'].quantile(0.25)
Q3 = df['Income'].quantile(0.75)
IQR = Q3 - Q1
upper_bound = Q3 + 1.5 * IQR

# 2. Option A â€“ Remove outliers above upper bound
df_no_outliers = df[df['Income'] <= upper_bound]

categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
mode_categorical = df[categorical_cols].mode().iloc[0]
print(mode_categorical)

# Scale numerical variables
#Select all numeric columns
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()

#Remove the target
numerical_cols.remove('AcceptedAnyCampaign')

import matplotlib.pyplot as plt

df[numerical_cols].boxplot(figsize=(12,6))
plt.xticks(rotation=45)
plt.title("Boxplots for Outlier Detection")
plt.show()

from scipy import stats

# Select the numerical columns from the DataFrame
df_numerical = df[numerical_cols]

z_scores = stats.zscore(df_numerical)
outliers = df_numerical[np.abs(z_scores) > 3]  # 3 is a common threshold
print(outliers)

import matplotlib.pyplot as plt
import pandas as pd


# Create boxplot
plt.boxplot(df['Income'])
plt.title("Boxplot of Data")
plt.ylabel("Values")
plt.show()

# Standarize the numerical variables
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])

#One-hot encoding for the categorical variables
df= pd.get_dummies(df, columns=['Education', 'Marital_Status'], drop_first=True)

"""#Applying the Logistic regression"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
X = df.drop('AcceptedAnyCampaign', axis=1)
y = df['AcceptedAnyCampaign']

#Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

model1 = LogisticRegression(max_iter=1000, random_state=42)
model1.fit(X_train, y_train)

#Predict on test set
y_pred = model1.predict(X_test)

# Evaluation the results
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\n Classification Report:")
print(classification_report(y_test, y_pred))

# STEP 4: Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("\n Confusion Matrix:")
print(cm)

"""#Random Forest Model

"""

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
param_grid = {
    'n_estimators': [200, 300, 400],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}

rf = RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,
                           cv=3, n_jobs=-1, scoring='f1', verbose=1)

grid_search.fit(X_train, y_train)
print("Best Parameters:", grid_search.best_params_)

# Train Random Forest
from sklearn.ensemble import RandomForestClassifier
model2 = RandomForestClassifier(n_estimators=400, max_depth=20,min_samples_split=2,min_samples_leaf=1, max_features='sqrt',random_state=42)
model2.fit(X_train, y_train)

# Predict
y_pred = model2.predict(X_test)

# Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Initialize Gradient Boosting model
model3 = GradientBoostingClassifier(n_estimators=300,learning_rate=0.1,max_depth=3,min_samples_split=5,min_samples_leaf=3,random_state=42)

# Train model
model3.fit(X_train, y_train)

# Predict
y_pred_model3 = model3.predict(X_test)

# Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred_model3))
print("\nClassification Report:\n", classification_report(y_test, y_pred_model3))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_model3))

from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix


model4 = XGBClassifier(n_estimators=300,learning_rate=0.1, max_depth=4,subsample=0.8,colsample_bytree=0.8,random_state=42,
 use_label_encoder=False,
    eval_metric='logloss'
)

# Train the model
model4.fit(X_train, y_train)

# Predict
y_pred_model4 = model4.predict(X_test)

# Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred_model4))
print("\nClassification Report:\n", classification_report(y_test, y_pred_model4))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_model4))
